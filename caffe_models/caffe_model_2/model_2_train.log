I0713 00:21:18.298820   316 caffe.cpp:211] Use CPU.
I0713 00:21:18.299283   316 solver.cpp:44] Initializing solver from parameters: 
test_iter: 1000
test_interval: 1000
base_lr: 0.001
display: 50
max_iter: 40000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 250
snapshot: 5000
snapshot_prefix: "/workspace/caffe_training/caffe_models/caffe_model_2/caffe_model_2"
solver_mode: CPU
net: "/workspace/caffe_training/caffe_models/caffe_model_2/caffenet_train_val_2.prototxt"
train_state {
  level: 0
  stage: ""
}
I0713 00:21:18.299551   316 solver.cpp:87] Creating training net from net file: /workspace/caffe_training/caffe_models/caffe_model_2/caffenet_train_val_2.prototxt
I0713 00:21:18.300619   316 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0713 00:21:18.300704   316 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0713 00:21:18.301170   316 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/workspace/caffe_training/input/mean.binaryproto"
  }
  data_param {
    source: "/workspace/caffe_training/input/train_lmdb"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-ones-tens"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-ones-tens"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-ones-tens"
  bottom: "label"
  top: "loss"
}
I0713 00:21:18.303339   316 layer_factory.hpp:77] Creating layer data
I0713 00:21:18.303860   316 db_lmdb.cpp:35] Opened lmdb /workspace/caffe_training/input/train_lmdb
I0713 00:21:18.303967   316 net.cpp:84] Creating Layer data
I0713 00:21:18.303997   316 net.cpp:380] data -> data
I0713 00:21:18.304128   316 net.cpp:380] data -> label
I0713 00:21:18.304265   316 data_transformer.cpp:25] Loading mean file from: /workspace/caffe_training/input/mean.binaryproto
I0713 00:21:18.307324   316 data_layer.cpp:45] output data size: 32,3,227,227
I0713 00:21:18.307643   316 net.cpp:122] Setting up data
I0713 00:21:18.307683   316 net.cpp:129] Top shape: 32 3 227 227 (4946784)
I0713 00:21:18.307780   316 net.cpp:129] Top shape: 32 (32)
I0713 00:21:18.307787   316 net.cpp:137] Memory required for data: 19787264
I0713 00:21:18.307812   316 layer_factory.hpp:77] Creating layer conv1
I0713 00:21:18.307886   316 net.cpp:84] Creating Layer conv1
I0713 00:21:18.307909   316 net.cpp:406] conv1 <- data
I0713 00:21:18.307927   316 net.cpp:380] conv1 -> conv1
I0713 00:21:18.308658   316 net.cpp:122] Setting up conv1
I0713 00:21:18.308748   316 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I0713 00:21:18.308768   316 net.cpp:137] Memory required for data: 56958464
I0713 00:21:18.308794   316 layer_factory.hpp:77] Creating layer relu1
I0713 00:21:18.308821   316 net.cpp:84] Creating Layer relu1
I0713 00:21:18.308838   316 net.cpp:406] relu1 <- conv1
I0713 00:21:18.308850   316 net.cpp:367] relu1 -> conv1 (in-place)
I0713 00:21:18.308872   316 net.cpp:122] Setting up relu1
I0713 00:21:18.308890   316 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I0713 00:21:18.308898   316 net.cpp:137] Memory required for data: 94129664
I0713 00:21:18.308907   316 layer_factory.hpp:77] Creating layer pool1
I0713 00:21:18.308917   316 net.cpp:84] Creating Layer pool1
I0713 00:21:18.308924   316 net.cpp:406] pool1 <- conv1
I0713 00:21:18.308933   316 net.cpp:380] pool1 -> pool1
I0713 00:21:18.308984   316 net.cpp:122] Setting up pool1
I0713 00:21:18.308995   316 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I0713 00:21:18.309123   316 net.cpp:137] Memory required for data: 103087616
I0713 00:21:18.309146   316 layer_factory.hpp:77] Creating layer norm1
I0713 00:21:18.309200   316 net.cpp:84] Creating Layer norm1
I0713 00:21:18.309208   316 net.cpp:406] norm1 <- pool1
I0713 00:21:18.309221   316 net.cpp:380] norm1 -> norm1
I0713 00:21:18.309238   316 net.cpp:122] Setting up norm1
I0713 00:21:18.309286   316 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I0713 00:21:18.309319   316 net.cpp:137] Memory required for data: 112045568
I0713 00:21:18.309331   316 layer_factory.hpp:77] Creating layer conv2
I0713 00:21:18.309366   316 net.cpp:84] Creating Layer conv2
I0713 00:21:18.309391   316 net.cpp:406] conv2 <- norm1
I0713 00:21:18.309408   316 net.cpp:380] conv2 -> conv2
I0713 00:21:18.312633   316 net.cpp:122] Setting up conv2
I0713 00:21:18.312674   316 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I0713 00:21:18.312685   316 net.cpp:137] Memory required for data: 135933440
I0713 00:21:18.312705   316 layer_factory.hpp:77] Creating layer relu2
I0713 00:21:18.312723   316 net.cpp:84] Creating Layer relu2
I0713 00:21:18.312734   316 net.cpp:406] relu2 <- conv2
I0713 00:21:18.312747   316 net.cpp:367] relu2 -> conv2 (in-place)
I0713 00:21:18.313097   316 net.cpp:122] Setting up relu2
I0713 00:21:18.313124   316 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I0713 00:21:18.313135   316 net.cpp:137] Memory required for data: 159821312
I0713 00:21:18.313246   316 layer_factory.hpp:77] Creating layer pool2
I0713 00:21:18.313258   316 net.cpp:84] Creating Layer pool2
I0713 00:21:18.313266   316 net.cpp:406] pool2 <- conv2
I0713 00:21:18.313421   316 net.cpp:380] pool2 -> pool2
I0713 00:21:18.313544   316 net.cpp:122] Setting up pool2
I0713 00:21:18.313561   316 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0713 00:21:18.313690   316 net.cpp:137] Memory required for data: 165359104
I0713 00:21:18.313797   316 layer_factory.hpp:77] Creating layer norm2
I0713 00:21:18.313817   316 net.cpp:84] Creating Layer norm2
I0713 00:21:18.313828   316 net.cpp:406] norm2 <- pool2
I0713 00:21:18.314174   316 net.cpp:380] norm2 -> norm2
I0713 00:21:18.314293   316 net.cpp:122] Setting up norm2
I0713 00:21:18.314328   316 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0713 00:21:18.314545   316 net.cpp:137] Memory required for data: 170896896
I0713 00:21:18.314579   316 layer_factory.hpp:77] Creating layer conv3
I0713 00:21:18.314724   316 net.cpp:84] Creating Layer conv3
I0713 00:21:18.314741   316 net.cpp:406] conv3 <- norm2
I0713 00:21:18.314757   316 net.cpp:380] conv3 -> conv3
I0713 00:21:18.324061   316 net.cpp:122] Setting up conv3
I0713 00:21:18.324115   316 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0713 00:21:18.324131   316 net.cpp:137] Memory required for data: 179203584
I0713 00:21:18.324152   316 layer_factory.hpp:77] Creating layer relu3
I0713 00:21:18.324169   316 net.cpp:84] Creating Layer relu3
I0713 00:21:18.324179   316 net.cpp:406] relu3 <- conv3
I0713 00:21:18.324193   316 net.cpp:367] relu3 -> conv3 (in-place)
I0713 00:21:18.324295   316 net.cpp:122] Setting up relu3
I0713 00:21:18.324388   316 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0713 00:21:18.324417   316 net.cpp:137] Memory required for data: 187510272
I0713 00:21:18.324674   316 layer_factory.hpp:77] Creating layer conv4
I0713 00:21:18.324965   316 net.cpp:84] Creating Layer conv4
I0713 00:21:18.324990   316 net.cpp:406] conv4 <- conv3
I0713 00:21:18.325129   316 net.cpp:380] conv4 -> conv4
I0713 00:21:18.332067   316 net.cpp:122] Setting up conv4
I0713 00:21:18.332128   316 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0713 00:21:18.332142   316 net.cpp:137] Memory required for data: 195816960
I0713 00:21:18.332161   316 layer_factory.hpp:77] Creating layer relu4
I0713 00:21:18.332181   316 net.cpp:84] Creating Layer relu4
I0713 00:21:18.332193   316 net.cpp:406] relu4 <- conv4
I0713 00:21:18.332207   316 net.cpp:367] relu4 -> conv4 (in-place)
I0713 00:21:18.332224   316 net.cpp:122] Setting up relu4
I0713 00:21:18.332235   316 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0713 00:21:18.332255   316 net.cpp:137] Memory required for data: 204123648
I0713 00:21:18.332345   316 layer_factory.hpp:77] Creating layer conv5
I0713 00:21:18.332671   316 net.cpp:84] Creating Layer conv5
I0713 00:21:18.332813   316 net.cpp:406] conv5 <- conv4
I0713 00:21:18.333025   316 net.cpp:380] conv5 -> conv5
I0713 00:21:18.339800   316 net.cpp:122] Setting up conv5
I0713 00:21:18.339884   316 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0713 00:21:18.339905   316 net.cpp:137] Memory required for data: 209661440
I0713 00:21:18.339938   316 layer_factory.hpp:77] Creating layer relu5
I0713 00:21:18.340052   316 net.cpp:84] Creating Layer relu5
I0713 00:21:18.340179   316 net.cpp:406] relu5 <- conv5
I0713 00:21:18.340430   316 net.cpp:367] relu5 -> conv5 (in-place)
I0713 00:21:18.340482   316 net.cpp:122] Setting up relu5
I0713 00:21:18.340502   316 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0713 00:21:18.340641   316 net.cpp:137] Memory required for data: 215199232
I0713 00:21:18.340658   316 layer_factory.hpp:77] Creating layer pool5
I0713 00:21:18.340678   316 net.cpp:84] Creating Layer pool5
I0713 00:21:18.340692   316 net.cpp:406] pool5 <- conv5
I0713 00:21:18.340709   316 net.cpp:380] pool5 -> pool5
I0713 00:21:18.340734   316 net.cpp:122] Setting up pool5
I0713 00:21:18.340751   316 net.cpp:129] Top shape: 32 256 6 6 (294912)
I0713 00:21:18.340764   316 net.cpp:137] Memory required for data: 216378880
I0713 00:21:18.340775   316 layer_factory.hpp:77] Creating layer fc6
I0713 00:21:18.340797   316 net.cpp:84] Creating Layer fc6
I0713 00:21:18.340812   316 net.cpp:406] fc6 <- pool5
I0713 00:21:18.340831   316 net.cpp:380] fc6 -> fc6
I0713 00:21:18.745002   316 net.cpp:122] Setting up fc6
I0713 00:21:18.745060   316 net.cpp:129] Top shape: 32 4096 (131072)
I0713 00:21:18.745070   316 net.cpp:137] Memory required for data: 216903168
I0713 00:21:18.745085   316 layer_factory.hpp:77] Creating layer relu6
I0713 00:21:18.745103   316 net.cpp:84] Creating Layer relu6
I0713 00:21:18.745112   316 net.cpp:406] relu6 <- fc6
I0713 00:21:18.745124   316 net.cpp:367] relu6 -> fc6 (in-place)
I0713 00:21:18.745138   316 net.cpp:122] Setting up relu6
I0713 00:21:18.745148   316 net.cpp:129] Top shape: 32 4096 (131072)
I0713 00:21:18.745157   316 net.cpp:137] Memory required for data: 217427456
I0713 00:21:18.745165   316 layer_factory.hpp:77] Creating layer drop6
I0713 00:21:18.745177   316 net.cpp:84] Creating Layer drop6
I0713 00:21:18.745185   316 net.cpp:406] drop6 <- fc6
I0713 00:21:18.745193   316 net.cpp:367] drop6 -> fc6 (in-place)
I0713 00:21:18.745208   316 net.cpp:122] Setting up drop6
I0713 00:21:18.745234   316 net.cpp:129] Top shape: 32 4096 (131072)
I0713 00:21:18.745285   316 net.cpp:137] Memory required for data: 217951744
I0713 00:21:18.745295   316 layer_factory.hpp:77] Creating layer fc7
I0713 00:21:18.745307   316 net.cpp:84] Creating Layer fc7
I0713 00:21:18.745316   316 net.cpp:406] fc7 <- fc6
I0713 00:21:18.745329   316 net.cpp:380] fc7 -> fc7
I0713 00:21:18.923367   316 net.cpp:122] Setting up fc7
I0713 00:21:18.923418   316 net.cpp:129] Top shape: 32 4096 (131072)
I0713 00:21:18.923427   316 net.cpp:137] Memory required for data: 218476032
I0713 00:21:18.923444   316 layer_factory.hpp:77] Creating layer relu7
I0713 00:21:18.923458   316 net.cpp:84] Creating Layer relu7
I0713 00:21:18.923467   316 net.cpp:406] relu7 <- fc7
I0713 00:21:18.923478   316 net.cpp:367] relu7 -> fc7 (in-place)
I0713 00:21:18.923491   316 net.cpp:122] Setting up relu7
I0713 00:21:18.923501   316 net.cpp:129] Top shape: 32 4096 (131072)
I0713 00:21:18.923508   316 net.cpp:137] Memory required for data: 219000320
I0713 00:21:18.923516   316 layer_factory.hpp:77] Creating layer drop7
I0713 00:21:18.923527   316 net.cpp:84] Creating Layer drop7
I0713 00:21:18.923534   316 net.cpp:406] drop7 <- fc7
I0713 00:21:18.923542   316 net.cpp:367] drop7 -> fc7 (in-place)
I0713 00:21:18.923552   316 net.cpp:122] Setting up drop7
I0713 00:21:18.923560   316 net.cpp:129] Top shape: 32 4096 (131072)
I0713 00:21:18.923568   316 net.cpp:137] Memory required for data: 219524608
I0713 00:21:18.923579   316 layer_factory.hpp:77] Creating layer fc8-ones-tens
I0713 00:21:18.923899   316 net.cpp:84] Creating Layer fc8-ones-tens
I0713 00:21:18.923918   316 net.cpp:406] fc8-ones-tens <- fc7
I0713 00:21:18.924065   316 net.cpp:380] fc8-ones-tens -> fc8-ones-tens
I0713 00:21:18.925151   316 net.cpp:122] Setting up fc8-ones-tens
I0713 00:21:18.925178   316 net.cpp:129] Top shape: 32 2 (64)
I0713 00:21:18.925187   316 net.cpp:137] Memory required for data: 219524864
I0713 00:21:18.925376   316 layer_factory.hpp:77] Creating layer loss
I0713 00:21:18.925467   316 net.cpp:84] Creating Layer loss
I0713 00:21:18.925480   316 net.cpp:406] loss <- fc8-ones-tens
I0713 00:21:18.925637   316 net.cpp:406] loss <- label
I0713 00:21:18.925757   316 net.cpp:380] loss -> loss
I0713 00:21:18.925932   316 layer_factory.hpp:77] Creating layer loss
I0713 00:21:18.926067   316 net.cpp:122] Setting up loss
I0713 00:21:18.926086   316 net.cpp:129] Top shape: (1)
I0713 00:21:18.926272   316 net.cpp:132]     with loss weight 1
I0713 00:21:18.926378   316 net.cpp:137] Memory required for data: 219524868
I0713 00:21:18.926391   316 net.cpp:198] loss needs backward computation.
I0713 00:21:18.926406   316 net.cpp:198] fc8-ones-tens needs backward computation.
I0713 00:21:18.926528   316 net.cpp:200] drop7 does not need backward computation.
I0713 00:21:18.926548   316 net.cpp:200] relu7 does not need backward computation.
I0713 00:21:18.926561   316 net.cpp:200] fc7 does not need backward computation.
I0713 00:21:18.926671   316 net.cpp:200] drop6 does not need backward computation.
I0713 00:21:18.926679   316 net.cpp:200] relu6 does not need backward computation.
I0713 00:21:18.926851   316 net.cpp:200] fc6 does not need backward computation.
I0713 00:21:18.926868   316 net.cpp:200] pool5 does not need backward computation.
I0713 00:21:18.926956   316 net.cpp:200] relu5 does not need backward computation.
I0713 00:21:18.926970   316 net.cpp:200] conv5 does not need backward computation.
I0713 00:21:18.926978   316 net.cpp:200] relu4 does not need backward computation.
I0713 00:21:18.926987   316 net.cpp:200] conv4 does not need backward computation.
I0713 00:21:18.927014   316 net.cpp:200] relu3 does not need backward computation.
I0713 00:21:18.927188   316 net.cpp:200] conv3 does not need backward computation.
I0713 00:21:18.927201   316 net.cpp:200] norm2 does not need backward computation.
I0713 00:21:18.927209   316 net.cpp:200] pool2 does not need backward computation.
I0713 00:21:18.927294   316 net.cpp:200] relu2 does not need backward computation.
I0713 00:21:18.927305   316 net.cpp:200] conv2 does not need backward computation.
I0713 00:21:18.927317   316 net.cpp:200] norm1 does not need backward computation.
I0713 00:21:18.927471   316 net.cpp:200] pool1 does not need backward computation.
I0713 00:21:18.927582   316 net.cpp:200] relu1 does not need backward computation.
I0713 00:21:18.927593   316 net.cpp:200] conv1 does not need backward computation.
I0713 00:21:18.927770   316 net.cpp:200] data does not need backward computation.
I0713 00:21:18.927780   316 net.cpp:242] This network produces output loss
I0713 00:21:18.927881   316 net.cpp:255] Network initialization done.
I0713 00:21:18.928966   316 solver.cpp:172] Creating test net (#0) specified by net file: /workspace/caffe_training/caffe_models/caffe_model_2/caffenet_train_val_2.prototxt
I0713 00:21:18.929070   316 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0713 00:21:18.929534   316 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/workspace/caffe_training/input/mean.binaryproto"
  }
  data_param {
    source: "/workspace/caffe_training/input/validation_lmdb"
    batch_size: 2
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-ones-tens"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-ones-tens"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8-ones-tens"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-ones-tens"
  bottom: "label"
  top: "loss"
}
I0713 00:21:18.929821   316 layer_factory.hpp:77] Creating layer data
I0713 00:21:18.930261   316 db_lmdb.cpp:35] Opened lmdb /workspace/caffe_training/input/validation_lmdb
I0713 00:21:18.930495   316 net.cpp:84] Creating Layer data
I0713 00:21:18.930621   316 net.cpp:380] data -> data
I0713 00:21:18.930650   316 net.cpp:380] data -> label
I0713 00:21:18.931042   316 data_transformer.cpp:25] Loading mean file from: /workspace/caffe_training/input/mean.binaryproto
I0713 00:21:18.936442   316 data_layer.cpp:45] output data size: 2,3,227,227
I0713 00:21:18.937044   316 net.cpp:122] Setting up data
I0713 00:21:18.937139   316 net.cpp:129] Top shape: 2 3 227 227 (309174)
I0713 00:21:18.937155   316 net.cpp:129] Top shape: 2 (2)
I0713 00:21:18.937168   316 net.cpp:137] Memory required for data: 1236704
I0713 00:21:18.937182   316 layer_factory.hpp:77] Creating layer label_data_1_split
I0713 00:21:18.937202   316 net.cpp:84] Creating Layer label_data_1_split
I0713 00:21:18.937213   316 net.cpp:406] label_data_1_split <- label
I0713 00:21:18.937235   316 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0713 00:21:18.937278   316 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0713 00:21:18.937413   316 net.cpp:122] Setting up label_data_1_split
I0713 00:21:18.937427   316 net.cpp:129] Top shape: 2 (2)
I0713 00:21:18.937436   316 net.cpp:129] Top shape: 2 (2)
I0713 00:21:18.937443   316 net.cpp:137] Memory required for data: 1236720
I0713 00:21:18.937453   316 layer_factory.hpp:77] Creating layer conv1
I0713 00:21:18.937477   316 net.cpp:84] Creating Layer conv1
I0713 00:21:18.937510   316 net.cpp:406] conv1 <- data
I0713 00:21:18.937564   316 net.cpp:380] conv1 -> conv1
I0713 00:21:18.938406   316 net.cpp:122] Setting up conv1
I0713 00:21:18.938504   316 net.cpp:129] Top shape: 2 96 55 55 (580800)
I0713 00:21:18.938524   316 net.cpp:137] Memory required for data: 3559920
I0713 00:21:18.938550   316 layer_factory.hpp:77] Creating layer relu1
I0713 00:21:18.938573   316 net.cpp:84] Creating Layer relu1
I0713 00:21:18.938637   316 net.cpp:406] relu1 <- conv1
I0713 00:21:18.938659   316 net.cpp:367] relu1 -> conv1 (in-place)
I0713 00:21:18.938680   316 net.cpp:122] Setting up relu1
I0713 00:21:18.938694   316 net.cpp:129] Top shape: 2 96 55 55 (580800)
I0713 00:21:18.938702   316 net.cpp:137] Memory required for data: 5883120
I0713 00:21:18.938710   316 layer_factory.hpp:77] Creating layer pool1
I0713 00:21:18.938788   316 net.cpp:84] Creating Layer pool1
I0713 00:21:18.938802   316 net.cpp:406] pool1 <- conv1
I0713 00:21:18.938817   316 net.cpp:380] pool1 -> pool1
I0713 00:21:18.938834   316 net.cpp:122] Setting up pool1
I0713 00:21:18.938850   316 net.cpp:129] Top shape: 2 96 27 27 (139968)
I0713 00:21:18.938886   316 net.cpp:137] Memory required for data: 6442992
I0713 00:21:18.938922   316 layer_factory.hpp:77] Creating layer norm1
I0713 00:21:18.938949   316 net.cpp:84] Creating Layer norm1
I0713 00:21:18.938963   316 net.cpp:406] norm1 <- pool1
I0713 00:21:18.938977   316 net.cpp:380] norm1 -> norm1
I0713 00:21:18.938992   316 net.cpp:122] Setting up norm1
I0713 00:21:18.939003   316 net.cpp:129] Top shape: 2 96 27 27 (139968)
I0713 00:21:18.939087   316 net.cpp:137] Memory required for data: 7002864
I0713 00:21:18.939100   316 layer_factory.hpp:77] Creating layer conv2
I0713 00:21:18.939126   316 net.cpp:84] Creating Layer conv2
I0713 00:21:18.939172   316 net.cpp:406] conv2 <- norm1
I0713 00:21:18.939209   316 net.cpp:380] conv2 -> conv2
I0713 00:21:18.945662   316 net.cpp:122] Setting up conv2
I0713 00:21:18.945782   316 net.cpp:129] Top shape: 2 256 27 27 (373248)
I0713 00:21:18.945793   316 net.cpp:137] Memory required for data: 8495856
I0713 00:21:18.945835   316 layer_factory.hpp:77] Creating layer relu2
I0713 00:21:18.945861   316 net.cpp:84] Creating Layer relu2
I0713 00:21:18.945927   316 net.cpp:406] relu2 <- conv2
I0713 00:21:18.945957   316 net.cpp:367] relu2 -> conv2 (in-place)
I0713 00:21:18.945974   316 net.cpp:122] Setting up relu2
I0713 00:21:18.945982   316 net.cpp:129] Top shape: 2 256 27 27 (373248)
I0713 00:21:18.945991   316 net.cpp:137] Memory required for data: 9988848
I0713 00:21:18.945999   316 layer_factory.hpp:77] Creating layer pool2
I0713 00:21:18.946017   316 net.cpp:84] Creating Layer pool2
I0713 00:21:18.946077   316 net.cpp:406] pool2 <- conv2
I0713 00:21:18.946159   316 net.cpp:380] pool2 -> pool2
I0713 00:21:18.946185   316 net.cpp:122] Setting up pool2
I0713 00:21:18.946200   316 net.cpp:129] Top shape: 2 256 13 13 (86528)
I0713 00:21:18.946213   316 net.cpp:137] Memory required for data: 10334960
I0713 00:21:18.946235   316 layer_factory.hpp:77] Creating layer norm2
I0713 00:21:18.946295   316 net.cpp:84] Creating Layer norm2
I0713 00:21:18.946311   316 net.cpp:406] norm2 <- pool2
I0713 00:21:18.946329   316 net.cpp:380] norm2 -> norm2
I0713 00:21:18.946346   316 net.cpp:122] Setting up norm2
I0713 00:21:18.946362   316 net.cpp:129] Top shape: 2 256 13 13 (86528)
I0713 00:21:18.946372   316 net.cpp:137] Memory required for data: 10681072
I0713 00:21:18.946458   316 layer_factory.hpp:77] Creating layer conv3
I0713 00:21:18.946483   316 net.cpp:84] Creating Layer conv3
I0713 00:21:18.946496   316 net.cpp:406] conv3 <- norm2
I0713 00:21:18.946513   316 net.cpp:380] conv3 -> conv3
I0713 00:21:18.960695   316 net.cpp:122] Setting up conv3
I0713 00:21:18.960840   316 net.cpp:129] Top shape: 2 384 13 13 (129792)
I0713 00:21:18.960880   316 net.cpp:137] Memory required for data: 11200240
I0713 00:21:18.960955   316 layer_factory.hpp:77] Creating layer relu3
I0713 00:21:18.961000   316 net.cpp:84] Creating Layer relu3
I0713 00:21:18.961158   316 net.cpp:406] relu3 <- conv3
I0713 00:21:18.961215   316 net.cpp:367] relu3 -> conv3 (in-place)
I0713 00:21:18.961252   316 net.cpp:122] Setting up relu3
I0713 00:21:18.961277   316 net.cpp:129] Top shape: 2 384 13 13 (129792)
I0713 00:21:18.961290   316 net.cpp:137] Memory required for data: 11719408
I0713 00:21:18.961372   316 layer_factory.hpp:77] Creating layer conv4
I0713 00:21:18.961468   316 net.cpp:84] Creating Layer conv4
I0713 00:21:18.961498   316 net.cpp:406] conv4 <- conv3
I0713 00:21:18.961539   316 net.cpp:380] conv4 -> conv4
I0713 00:21:18.974416   316 net.cpp:122] Setting up conv4
I0713 00:21:18.974510   316 net.cpp:129] Top shape: 2 384 13 13 (129792)
I0713 00:21:18.974524   316 net.cpp:137] Memory required for data: 12238576
I0713 00:21:18.974541   316 layer_factory.hpp:77] Creating layer relu4
I0713 00:21:18.974556   316 net.cpp:84] Creating Layer relu4
I0713 00:21:18.974647   316 net.cpp:406] relu4 <- conv4
I0713 00:21:18.974663   316 net.cpp:367] relu4 -> conv4 (in-place)
I0713 00:21:18.974722   316 net.cpp:122] Setting up relu4
I0713 00:21:18.974822   316 net.cpp:129] Top shape: 2 384 13 13 (129792)
I0713 00:21:18.974833   316 net.cpp:137] Memory required for data: 12757744
I0713 00:21:18.974841   316 layer_factory.hpp:77] Creating layer conv5
I0713 00:21:18.974862   316 net.cpp:84] Creating Layer conv5
I0713 00:21:18.974872   316 net.cpp:406] conv5 <- conv4
I0713 00:21:18.975128   316 net.cpp:380] conv5 -> conv5
I0713 00:21:18.983911   316 net.cpp:122] Setting up conv5
I0713 00:21:18.983971   316 net.cpp:129] Top shape: 2 256 13 13 (86528)
I0713 00:21:18.983983   316 net.cpp:137] Memory required for data: 13103856
I0713 00:21:18.984007   316 layer_factory.hpp:77] Creating layer relu5
I0713 00:21:18.984025   316 net.cpp:84] Creating Layer relu5
I0713 00:21:18.984035   316 net.cpp:406] relu5 <- conv5
I0713 00:21:18.984047   316 net.cpp:367] relu5 -> conv5 (in-place)
I0713 00:21:18.984062   316 net.cpp:122] Setting up relu5
I0713 00:21:18.984143   316 net.cpp:129] Top shape: 2 256 13 13 (86528)
I0713 00:21:18.984153   316 net.cpp:137] Memory required for data: 13449968
I0713 00:21:18.984197   316 layer_factory.hpp:77] Creating layer pool5
I0713 00:21:18.984212   316 net.cpp:84] Creating Layer pool5
I0713 00:21:18.984222   316 net.cpp:406] pool5 <- conv5
I0713 00:21:18.984232   316 net.cpp:380] pool5 -> pool5
I0713 00:21:18.984247   316 net.cpp:122] Setting up pool5
I0713 00:21:18.984256   316 net.cpp:129] Top shape: 2 256 6 6 (18432)
I0713 00:21:18.984264   316 net.cpp:137] Memory required for data: 13523696
I0713 00:21:18.984300   316 layer_factory.hpp:77] Creating layer fc6
I0713 00:21:18.984333   316 net.cpp:84] Creating Layer fc6
I0713 00:21:18.984338   316 net.cpp:406] fc6 <- pool5
I0713 00:21:18.984349   316 net.cpp:380] fc6 -> fc6
I0713 00:21:19.374615   316 net.cpp:122] Setting up fc6
I0713 00:21:19.374667   316 net.cpp:129] Top shape: 2 4096 (8192)
I0713 00:21:19.374676   316 net.cpp:137] Memory required for data: 13556464
I0713 00:21:19.374689   316 layer_factory.hpp:77] Creating layer relu6
I0713 00:21:19.374704   316 net.cpp:84] Creating Layer relu6
I0713 00:21:19.374714   316 net.cpp:406] relu6 <- fc6
I0713 00:21:19.374727   316 net.cpp:367] relu6 -> fc6 (in-place)
I0713 00:21:19.374739   316 net.cpp:122] Setting up relu6
I0713 00:21:19.374748   316 net.cpp:129] Top shape: 2 4096 (8192)
I0713 00:21:19.374755   316 net.cpp:137] Memory required for data: 13589232
I0713 00:21:19.374763   316 layer_factory.hpp:77] Creating layer drop6
I0713 00:21:19.374773   316 net.cpp:84] Creating Layer drop6
I0713 00:21:19.374917   316 net.cpp:406] drop6 <- fc6
I0713 00:21:19.374927   316 net.cpp:367] drop6 -> fc6 (in-place)
I0713 00:21:19.374940   316 net.cpp:122] Setting up drop6
I0713 00:21:19.374949   316 net.cpp:129] Top shape: 2 4096 (8192)
I0713 00:21:19.374955   316 net.cpp:137] Memory required for data: 13622000
I0713 00:21:19.374963   316 layer_factory.hpp:77] Creating layer fc7
I0713 00:21:19.374970   316 net.cpp:84] Creating Layer fc7
I0713 00:21:19.374976   316 net.cpp:406] fc7 <- fc6
I0713 00:21:19.374989   316 net.cpp:380] fc7 -> fc7
I0713 00:21:19.553601   316 net.cpp:122] Setting up fc7
I0713 00:21:19.553658   316 net.cpp:129] Top shape: 2 4096 (8192)
I0713 00:21:19.553668   316 net.cpp:137] Memory required for data: 13654768
I0713 00:21:19.553684   316 layer_factory.hpp:77] Creating layer relu7
I0713 00:21:19.553709   316 net.cpp:84] Creating Layer relu7
I0713 00:21:19.553720   316 net.cpp:406] relu7 <- fc7
I0713 00:21:19.553733   316 net.cpp:367] relu7 -> fc7 (in-place)
I0713 00:21:19.553747   316 net.cpp:122] Setting up relu7
I0713 00:21:19.553756   316 net.cpp:129] Top shape: 2 4096 (8192)
I0713 00:21:19.553764   316 net.cpp:137] Memory required for data: 13687536
I0713 00:21:19.553771   316 layer_factory.hpp:77] Creating layer drop7
I0713 00:21:19.553782   316 net.cpp:84] Creating Layer drop7
I0713 00:21:19.553791   316 net.cpp:406] drop7 <- fc7
I0713 00:21:19.553802   316 net.cpp:367] drop7 -> fc7 (in-place)
I0713 00:21:19.553885   316 net.cpp:122] Setting up drop7
I0713 00:21:19.553894   316 net.cpp:129] Top shape: 2 4096 (8192)
I0713 00:21:19.553902   316 net.cpp:137] Memory required for data: 13720304
I0713 00:21:19.553910   316 layer_factory.hpp:77] Creating layer fc8-ones-tens
I0713 00:21:19.553921   316 net.cpp:84] Creating Layer fc8-ones-tens
I0713 00:21:19.553930   316 net.cpp:406] fc8-ones-tens <- fc7
I0713 00:21:19.554020   316 net.cpp:380] fc8-ones-tens -> fc8-ones-tens
I0713 00:21:19.554886   316 net.cpp:122] Setting up fc8-ones-tens
I0713 00:21:19.554913   316 net.cpp:129] Top shape: 2 2 (4)
I0713 00:21:19.555171   316 net.cpp:137] Memory required for data: 13720320
I0713 00:21:19.555194   316 layer_factory.hpp:77] Creating layer fc8-ones-tens_fc8-ones-tens_0_split
I0713 00:21:19.555284   316 net.cpp:84] Creating Layer fc8-ones-tens_fc8-ones-tens_0_split
I0713 00:21:19.555294   316 net.cpp:406] fc8-ones-tens_fc8-ones-tens_0_split <- fc8-ones-tens
I0713 00:21:19.555308   316 net.cpp:380] fc8-ones-tens_fc8-ones-tens_0_split -> fc8-ones-tens_fc8-ones-tens_0_split_0
I0713 00:21:19.555528   316 net.cpp:380] fc8-ones-tens_fc8-ones-tens_0_split -> fc8-ones-tens_fc8-ones-tens_0_split_1
I0713 00:21:19.555644   316 net.cpp:122] Setting up fc8-ones-tens_fc8-ones-tens_0_split
I0713 00:21:19.555657   316 net.cpp:129] Top shape: 2 2 (4)
I0713 00:21:19.555673   316 net.cpp:129] Top shape: 2 2 (4)
I0713 00:21:19.555922   316 net.cpp:137] Memory required for data: 13720352
I0713 00:21:19.555933   316 layer_factory.hpp:77] Creating layer accuracy
I0713 00:21:19.555953   316 net.cpp:84] Creating Layer accuracy
I0713 00:21:19.555963   316 net.cpp:406] accuracy <- fc8-ones-tens_fc8-ones-tens_0_split_0
I0713 00:21:19.556203   316 net.cpp:406] accuracy <- label_data_1_split_0
I0713 00:21:19.556221   316 net.cpp:380] accuracy -> accuracy
I0713 00:21:19.556236   316 net.cpp:122] Setting up accuracy
I0713 00:21:19.556244   316 net.cpp:129] Top shape: (1)
I0713 00:21:19.556468   316 net.cpp:137] Memory required for data: 13720356
I0713 00:21:19.556483   316 layer_factory.hpp:77] Creating layer loss
I0713 00:21:19.556618   316 net.cpp:84] Creating Layer loss
I0713 00:21:19.556710   316 net.cpp:406] loss <- fc8-ones-tens_fc8-ones-tens_0_split_1
I0713 00:21:19.556722   316 net.cpp:406] loss <- label_data_1_split_1
I0713 00:21:19.556733   316 net.cpp:380] loss -> loss
I0713 00:21:19.556876   316 layer_factory.hpp:77] Creating layer loss
I0713 00:21:19.557018   316 net.cpp:122] Setting up loss
I0713 00:21:19.557104   316 net.cpp:129] Top shape: (1)
I0713 00:21:19.557337   316 net.cpp:132]     with loss weight 1
I0713 00:21:19.557428   316 net.cpp:137] Memory required for data: 13720360
I0713 00:21:19.557440   316 net.cpp:198] loss needs backward computation.
I0713 00:21:19.557449   316 net.cpp:200] accuracy does not need backward computation.
I0713 00:21:19.557457   316 net.cpp:198] fc8-ones-tens_fc8-ones-tens_0_split needs backward computation.
I0713 00:21:19.557466   316 net.cpp:198] fc8-ones-tens needs backward computation.
I0713 00:21:19.557479   316 net.cpp:200] drop7 does not need backward computation.
I0713 00:21:19.557780   316 net.cpp:200] relu7 does not need backward computation.
I0713 00:21:19.557788   316 net.cpp:200] fc7 does not need backward computation.
I0713 00:21:19.557796   316 net.cpp:200] drop6 does not need backward computation.
I0713 00:21:19.557804   316 net.cpp:200] relu6 does not need backward computation.
I0713 00:21:19.557812   316 net.cpp:200] fc6 does not need backward computation.
I0713 00:21:19.558013   316 net.cpp:200] pool5 does not need backward computation.
I0713 00:21:19.558027   316 net.cpp:200] relu5 does not need backward computation.
I0713 00:21:19.558037   316 net.cpp:200] conv5 does not need backward computation.
I0713 00:21:19.558043   316 net.cpp:200] relu4 does not need backward computation.
I0713 00:21:19.558053   316 net.cpp:200] conv4 does not need backward computation.
I0713 00:21:19.558197   316 net.cpp:200] relu3 does not need backward computation.
I0713 00:21:19.558289   316 net.cpp:200] conv3 does not need backward computation.
I0713 00:21:19.558303   316 net.cpp:200] norm2 does not need backward computation.
I0713 00:21:19.558312   316 net.cpp:200] pool2 does not need backward computation.
I0713 00:21:19.558320   316 net.cpp:200] relu2 does not need backward computation.
I0713 00:21:19.558324   316 net.cpp:200] conv2 does not need backward computation.
I0713 00:21:19.558334   316 net.cpp:200] norm1 does not need backward computation.
I0713 00:21:19.558589   316 net.cpp:200] pool1 does not need backward computation.
I0713 00:21:19.558605   316 net.cpp:200] relu1 does not need backward computation.
I0713 00:21:19.558610   316 net.cpp:200] conv1 does not need backward computation.
I0713 00:21:19.558619   316 net.cpp:200] label_data_1_split does not need backward computation.
I0713 00:21:19.558857   316 net.cpp:200] data does not need backward computation.
I0713 00:21:19.558868   316 net.cpp:242] This network produces output accuracy
I0713 00:21:19.558959   316 net.cpp:242] This network produces output loss
I0713 00:21:19.558989   316 net.cpp:255] Network initialization done.
I0713 00:21:19.559386   316 solver.cpp:56] Solver scaffolding done.
I0713 00:21:19.559578   316 caffe.cpp:155] Finetuning from ../bvlc_reference_caffenet.caffemodel
I0713 00:21:19.819407   316 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../bvlc_reference_caffenet.caffemodel
I0713 00:21:19.819463   316 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0713 00:21:19.819476   316 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0713 00:21:19.819700   316 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../bvlc_reference_caffenet.caffemodel
I0713 00:21:20.061743   316 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0713 00:21:20.167569   316 net.cpp:744] Ignoring source layer fc8
I0713 00:21:20.381135   316 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../bvlc_reference_caffenet.caffemodel
I0713 00:21:20.381196   316 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0713 00:21:20.381217   316 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0713 00:21:20.381247   316 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../bvlc_reference_caffenet.caffemodel
I0713 00:21:20.566488   316 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0713 00:21:20.663975   316 net.cpp:744] Ignoring source layer fc8
I0713 00:21:20.755625   316 caffe.cpp:248] Starting Optimization
I0713 00:21:20.755694   316 solver.cpp:272] Solving CaffeNet
I0713 00:21:20.755709   316 solver.cpp:273] Learning Rate Policy: step
I0713 00:21:20.840937   316 solver.cpp:330] Iteration 0, Testing net (#0)
I0713 00:21:26.455595   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:21:33.133035   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:21:40.393373   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:21:47.616961   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:21:54.428004   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:02.568691   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:10.494186   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:17.173393   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:24.112241   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:31.574162   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:40.080668   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:47.567683   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:22:54.393543   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:01.107547   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:07.892688   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:14.760180   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:22.398808   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:30.975949   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:37.580273   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:44.311167   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:50.929940   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:23:57.679036   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:04.304455   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:10.886873   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:17.441028   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:24.215313   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:33.480270   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:41.106611   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:48.529013   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:24:56.367573   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:06.512917   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:14.664808   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:22.121142   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:29.182209   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:36.022337   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:43.297049   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:50.461488   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:25:57.117414   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:26:03.831650   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:26:10.510027   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:26:17.221530   320 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:26:22.865383   316 solver.cpp:397]     Test net output #0: accuracy = 0.062
I0713 00:26:22.865460   316 solver.cpp:397]     Test net output #1: loss = 38.3424 (* 1 = 38.3424 loss)
I0713 00:26:26.462842   316 solver.cpp:218] Iteration 0 (0 iter/s, 305.707s/50 iters), loss = 0.755594
I0713 00:26:26.462924   316 solver.cpp:237]     Train net output #0: loss = 0.755594 (* 1 = 0.755594 loss)
I0713 00:26:26.462944   316 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0713 00:26:44.827080   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:27:19.817241   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:27:54.380935   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:28:24.717974   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:28:56.511168   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:29:33.739792   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:29:48.879614   316 solver.cpp:218] Iteration 50 (0.247016 iter/s, 202.416s/50 iters), loss = 6.00177
I0713 00:29:48.879741   316 solver.cpp:237]     Train net output #0: loss = 6.00177 (* 1 = 6.00177 loss)
I0713 00:29:48.879763   316 sgd_solver.cpp:105] Iteration 50, lr = 0.001
I0713 00:30:07.727813   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:30:38.238514   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:31:12.566140   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:31:43.866822   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:32:16.130386   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:32:52.523097   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:33:07.726593   316 solver.cpp:218] Iteration 100 (0.251451 iter/s, 198.846s/50 iters), loss = 6.32307
I0713 00:33:07.726665   316 solver.cpp:237]     Train net output #0: loss = 6.32307 (* 1 = 6.32307 loss)
I0713 00:33:07.726678   316 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0713 00:33:23.361160   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:33:53.803467   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:34:28.589431   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:34:59.059896   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:35:30.919070   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:36:05.447749   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:36:21.550712   316 solver.cpp:218] Iteration 150 (0.257966 iter/s, 193.824s/50 iters), loss = 6.33905
I0713 00:36:21.550824   316 solver.cpp:237]     Train net output #0: loss = 6.33905 (* 1 = 6.33905 loss)
I0713 00:36:21.550843   316 sgd_solver.cpp:105] Iteration 150, lr = 0.001
I0713 00:36:38.565418   319 data_layer.cpp:73] Restarting data prefetching from start.
I0713 00:37:10.855742   319 data_layer.cpp:73] Restarting data prefetching from start.
