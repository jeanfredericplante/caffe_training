I0714 02:48:07.745831    11 caffe.cpp:211] Use CPU.
I0714 02:48:07.749244    11 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 1200
base_lr: 0.01
display: 50
max_iter: 6000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1000
snapshot: 2000
snapshot_prefix: "/workspace/caffe_training/caffe_models/caffe_model_2/caffe_model_2"
solver_mode: CPU
net: "/workspace/caffe_training/caffe_models/caffe_model_2/caffenet_train_val_2.prototxt"
train_state {
  level: 0
  stage: ""
}
I0714 02:48:07.749923    11 solver.cpp:87] Creating training net from net file: /workspace/caffe_training/caffe_models/caffe_model_2/caffenet_train_val_2.prototxt
I0714 02:48:07.750979    11 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0714 02:48:07.751029    11 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0714 02:48:07.751365    11 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/workspace/caffe_training/input/mean.binaryproto"
  }
  data_param {
    source: "/workspace/caffe_training/input/train_lmdb"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-ones-tens"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-ones-tens"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-ones-tens"
  bottom: "label"
  top: "loss"
}
I0714 02:48:07.751549    11 layer_factory.hpp:77] Creating layer data
I0714 02:48:07.756121    11 db_lmdb.cpp:35] Opened lmdb /workspace/caffe_training/input/train_lmdb
I0714 02:48:07.757190    11 net.cpp:84] Creating Layer data
I0714 02:48:07.757274    11 net.cpp:380] data -> data
I0714 02:48:07.757328    11 net.cpp:380] data -> label
I0714 02:48:07.757500    11 data_transformer.cpp:25] Loading mean file from: /workspace/caffe_training/input/mean.binaryproto
I0714 02:48:07.764583    11 data_layer.cpp:45] output data size: 32,3,227,227
I0714 02:48:07.764894    11 net.cpp:122] Setting up data
I0714 02:48:07.765125    11 net.cpp:129] Top shape: 32 3 227 227 (4946784)
I0714 02:48:07.765146    11 net.cpp:129] Top shape: 32 (32)
I0714 02:48:07.765154    11 net.cpp:137] Memory required for data: 19787264
I0714 02:48:07.765179    11 layer_factory.hpp:77] Creating layer conv1
I0714 02:48:07.765215    11 net.cpp:84] Creating Layer conv1
I0714 02:48:07.765241    11 net.cpp:406] conv1 <- data
I0714 02:48:07.765280    11 net.cpp:380] conv1 -> conv1
I0714 02:48:07.766393    11 net.cpp:122] Setting up conv1
I0714 02:48:07.766703    11 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I0714 02:48:07.766809    11 net.cpp:137] Memory required for data: 56958464
I0714 02:48:07.766855    11 layer_factory.hpp:77] Creating layer relu1
I0714 02:48:07.766871    11 net.cpp:84] Creating Layer relu1
I0714 02:48:07.766880    11 net.cpp:406] relu1 <- conv1
I0714 02:48:07.766892    11 net.cpp:367] relu1 -> conv1 (in-place)
I0714 02:48:07.766906    11 net.cpp:122] Setting up relu1
I0714 02:48:07.766933    11 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I0714 02:48:07.766973    11 net.cpp:137] Memory required for data: 94129664
I0714 02:48:07.766993    11 layer_factory.hpp:77] Creating layer pool1
I0714 02:48:07.767019    11 net.cpp:84] Creating Layer pool1
I0714 02:48:07.767045    11 net.cpp:406] pool1 <- conv1
I0714 02:48:07.767060    11 net.cpp:380] pool1 -> pool1
I0714 02:48:07.767117    11 net.cpp:122] Setting up pool1
I0714 02:48:07.767141    11 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I0714 02:48:07.767149    11 net.cpp:137] Memory required for data: 103087616
I0714 02:48:07.767158    11 layer_factory.hpp:77] Creating layer norm1
I0714 02:48:07.767199    11 net.cpp:84] Creating Layer norm1
I0714 02:48:07.767207    11 net.cpp:406] norm1 <- pool1
I0714 02:48:07.767217    11 net.cpp:380] norm1 -> norm1
I0714 02:48:07.767246    11 net.cpp:122] Setting up norm1
I0714 02:48:07.767267    11 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I0714 02:48:07.767274    11 net.cpp:137] Memory required for data: 112045568
I0714 02:48:07.767283    11 layer_factory.hpp:77] Creating layer conv2
I0714 02:48:07.767304    11 net.cpp:84] Creating Layer conv2
I0714 02:48:07.767328    11 net.cpp:406] conv2 <- norm1
I0714 02:48:07.767349    11 net.cpp:380] conv2 -> conv2
I0714 02:48:07.772120    11 net.cpp:122] Setting up conv2
I0714 02:48:07.772166    11 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I0714 02:48:07.772177    11 net.cpp:137] Memory required for data: 135933440
I0714 02:48:07.772194    11 layer_factory.hpp:77] Creating layer relu2
I0714 02:48:07.772209    11 net.cpp:84] Creating Layer relu2
I0714 02:48:07.772218    11 net.cpp:406] relu2 <- conv2
I0714 02:48:07.772230    11 net.cpp:367] relu2 -> conv2 (in-place)
I0714 02:48:07.773008    11 net.cpp:122] Setting up relu2
I0714 02:48:07.773128    11 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I0714 02:48:07.773169    11 net.cpp:137] Memory required for data: 159821312
I0714 02:48:07.773190    11 layer_factory.hpp:77] Creating layer pool2
I0714 02:48:07.773212    11 net.cpp:84] Creating Layer pool2
I0714 02:48:07.773226    11 net.cpp:406] pool2 <- conv2
I0714 02:48:07.773392    11 net.cpp:380] pool2 -> pool2
I0714 02:48:07.773542    11 net.cpp:122] Setting up pool2
I0714 02:48:07.773761    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:07.773787    11 net.cpp:137] Memory required for data: 165359104
I0714 02:48:07.773797    11 layer_factory.hpp:77] Creating layer norm2
I0714 02:48:07.773903    11 net.cpp:84] Creating Layer norm2
I0714 02:48:07.773941    11 net.cpp:406] norm2 <- pool2
I0714 02:48:07.773968    11 net.cpp:380] norm2 -> norm2
I0714 02:48:07.774194    11 net.cpp:122] Setting up norm2
I0714 02:48:07.774314    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:07.774336    11 net.cpp:137] Memory required for data: 170896896
I0714 02:48:07.774344    11 layer_factory.hpp:77] Creating layer conv3
I0714 02:48:07.774363    11 net.cpp:84] Creating Layer conv3
I0714 02:48:07.774435    11 net.cpp:406] conv3 <- norm2
I0714 02:48:07.774587    11 net.cpp:380] conv3 -> conv3
I0714 02:48:07.789506    11 net.cpp:122] Setting up conv3
I0714 02:48:07.789574    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:07.789589    11 net.cpp:137] Memory required for data: 179203584
I0714 02:48:07.789616    11 layer_factory.hpp:77] Creating layer relu3
I0714 02:48:07.789636    11 net.cpp:84] Creating Layer relu3
I0714 02:48:07.789888    11 net.cpp:406] relu3 <- conv3
I0714 02:48:07.790026    11 net.cpp:367] relu3 -> conv3 (in-place)
I0714 02:48:07.790134    11 net.cpp:122] Setting up relu3
I0714 02:48:07.790233    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:07.790246    11 net.cpp:137] Memory required for data: 187510272
I0714 02:48:07.790258    11 layer_factory.hpp:77] Creating layer conv4
I0714 02:48:07.790280    11 net.cpp:84] Creating Layer conv4
I0714 02:48:07.790428    11 net.cpp:406] conv4 <- conv3
I0714 02:48:07.790446    11 net.cpp:380] conv4 -> conv4
I0714 02:48:07.806838    11 net.cpp:122] Setting up conv4
I0714 02:48:07.806913    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:07.806933    11 net.cpp:137] Memory required for data: 195816960
I0714 02:48:07.806962    11 layer_factory.hpp:77] Creating layer relu4
I0714 02:48:07.807126    11 net.cpp:84] Creating Layer relu4
I0714 02:48:07.807226    11 net.cpp:406] relu4 <- conv4
I0714 02:48:07.807256    11 net.cpp:367] relu4 -> conv4 (in-place)
I0714 02:48:07.807389    11 net.cpp:122] Setting up relu4
I0714 02:48:07.807411    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:07.807440    11 net.cpp:137] Memory required for data: 204123648
I0714 02:48:07.807454    11 layer_factory.hpp:77] Creating layer conv5
I0714 02:48:07.807750    11 net.cpp:84] Creating Layer conv5
I0714 02:48:07.808120    11 net.cpp:406] conv5 <- conv4
I0714 02:48:07.809693    11 net.cpp:380] conv5 -> conv5
I0714 02:48:07.823535    11 net.cpp:122] Setting up conv5
I0714 02:48:07.823612    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:07.823639    11 net.cpp:137] Memory required for data: 209661440
I0714 02:48:07.823671    11 layer_factory.hpp:77] Creating layer relu5
I0714 02:48:07.823694    11 net.cpp:84] Creating Layer relu5
I0714 02:48:07.823712    11 net.cpp:406] relu5 <- conv5
I0714 02:48:07.823915    11 net.cpp:367] relu5 -> conv5 (in-place)
I0714 02:48:07.824337    11 net.cpp:122] Setting up relu5
I0714 02:48:07.824470    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:07.824496    11 net.cpp:137] Memory required for data: 215199232
I0714 02:48:07.824524    11 layer_factory.hpp:77] Creating layer pool5
I0714 02:48:07.824568    11 net.cpp:84] Creating Layer pool5
I0714 02:48:07.824594    11 net.cpp:406] pool5 <- conv5
I0714 02:48:07.824627    11 net.cpp:380] pool5 -> pool5
I0714 02:48:07.824687    11 net.cpp:122] Setting up pool5
I0714 02:48:07.825000    11 net.cpp:129] Top shape: 32 256 6 6 (294912)
I0714 02:48:07.825047    11 net.cpp:137] Memory required for data: 216378880
I0714 02:48:07.825072    11 layer_factory.hpp:77] Creating layer fc6
I0714 02:48:07.825119    11 net.cpp:84] Creating Layer fc6
I0714 02:48:07.825135    11 net.cpp:406] fc6 <- pool5
I0714 02:48:07.825155    11 net.cpp:380] fc6 -> fc6
I0714 02:48:08.376710    11 net.cpp:122] Setting up fc6
I0714 02:48:08.376754    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:08.376763    11 net.cpp:137] Memory required for data: 216903168
I0714 02:48:08.376781    11 layer_factory.hpp:77] Creating layer relu6
I0714 02:48:08.376794    11 net.cpp:84] Creating Layer relu6
I0714 02:48:08.376806    11 net.cpp:406] relu6 <- fc6
I0714 02:48:08.376821    11 net.cpp:367] relu6 -> fc6 (in-place)
I0714 02:48:08.376833    11 net.cpp:122] Setting up relu6
I0714 02:48:08.376842    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:08.376849    11 net.cpp:137] Memory required for data: 217427456
I0714 02:48:08.376917    11 layer_factory.hpp:77] Creating layer drop6
I0714 02:48:08.376932    11 net.cpp:84] Creating Layer drop6
I0714 02:48:08.376942    11 net.cpp:406] drop6 <- fc6
I0714 02:48:08.377008    11 net.cpp:367] drop6 -> fc6 (in-place)
I0714 02:48:08.377043    11 net.cpp:122] Setting up drop6
I0714 02:48:08.377053    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:08.377059    11 net.cpp:137] Memory required for data: 217951744
I0714 02:48:08.377073    11 layer_factory.hpp:77] Creating layer fc7
I0714 02:48:08.377089    11 net.cpp:84] Creating Layer fc7
I0714 02:48:08.377121    11 net.cpp:406] fc7 <- fc6
I0714 02:48:08.377168    11 net.cpp:380] fc7 -> fc7
I0714 02:48:08.613853    11 net.cpp:122] Setting up fc7
I0714 02:48:08.613903    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:08.613914    11 net.cpp:137] Memory required for data: 218476032
I0714 02:48:08.613929    11 layer_factory.hpp:77] Creating layer relu7
I0714 02:48:08.613943    11 net.cpp:84] Creating Layer relu7
I0714 02:48:08.613951    11 net.cpp:406] relu7 <- fc7
I0714 02:48:08.613963    11 net.cpp:367] relu7 -> fc7 (in-place)
I0714 02:48:08.613976    11 net.cpp:122] Setting up relu7
I0714 02:48:08.613984    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:08.613992    11 net.cpp:137] Memory required for data: 219000320
I0714 02:48:08.614001    11 layer_factory.hpp:77] Creating layer drop7
I0714 02:48:08.614011    11 net.cpp:84] Creating Layer drop7
I0714 02:48:08.614136    11 net.cpp:406] drop7 <- fc7
I0714 02:48:08.614156    11 net.cpp:367] drop7 -> fc7 (in-place)
I0714 02:48:08.614169    11 net.cpp:122] Setting up drop7
I0714 02:48:08.614179    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:08.614187    11 net.cpp:137] Memory required for data: 219524608
I0714 02:48:08.614193    11 layer_factory.hpp:77] Creating layer fc8-ones-tens
I0714 02:48:08.614204    11 net.cpp:84] Creating Layer fc8-ones-tens
I0714 02:48:08.614243    11 net.cpp:406] fc8-ones-tens <- fc7
I0714 02:48:08.614284    11 net.cpp:380] fc8-ones-tens -> fc8-ones-tens
I0714 02:48:08.614423    11 net.cpp:122] Setting up fc8-ones-tens
I0714 02:48:08.614434    11 net.cpp:129] Top shape: 32 2 (64)
I0714 02:48:08.614439    11 net.cpp:137] Memory required for data: 219524864
I0714 02:48:08.614454    11 layer_factory.hpp:77] Creating layer loss
I0714 02:48:08.614480    11 net.cpp:84] Creating Layer loss
I0714 02:48:08.614492    11 net.cpp:406] loss <- fc8-ones-tens
I0714 02:48:08.614511    11 net.cpp:406] loss <- label
I0714 02:48:08.614531    11 net.cpp:380] loss -> loss
I0714 02:48:08.614585    11 layer_factory.hpp:77] Creating layer loss
I0714 02:48:08.614624    11 net.cpp:122] Setting up loss
I0714 02:48:08.614650    11 net.cpp:129] Top shape: (1)
I0714 02:48:08.614715    11 net.cpp:132]     with loss weight 1
I0714 02:48:08.614778    11 net.cpp:137] Memory required for data: 219524868
I0714 02:48:08.614791    11 net.cpp:198] loss needs backward computation.
I0714 02:48:08.614820    11 net.cpp:198] fc8-ones-tens needs backward computation.
I0714 02:48:08.614831    11 net.cpp:200] drop7 does not need backward computation.
I0714 02:48:08.614841    11 net.cpp:200] relu7 does not need backward computation.
I0714 02:48:08.614850    11 net.cpp:200] fc7 does not need backward computation.
I0714 02:48:08.614858    11 net.cpp:200] drop6 does not need backward computation.
I0714 02:48:08.614866    11 net.cpp:200] relu6 does not need backward computation.
I0714 02:48:08.614871    11 net.cpp:200] fc6 does not need backward computation.
I0714 02:48:08.614878    11 net.cpp:200] pool5 does not need backward computation.
I0714 02:48:08.614887    11 net.cpp:200] relu5 does not need backward computation.
I0714 02:48:08.614897    11 net.cpp:200] conv5 does not need backward computation.
I0714 02:48:08.614920    11 net.cpp:200] relu4 does not need backward computation.
I0714 02:48:08.614929    11 net.cpp:200] conv4 does not need backward computation.
I0714 02:48:08.614936    11 net.cpp:200] relu3 does not need backward computation.
I0714 02:48:08.614941    11 net.cpp:200] conv3 does not need backward computation.
I0714 02:48:08.614948    11 net.cpp:200] norm2 does not need backward computation.
I0714 02:48:08.614955    11 net.cpp:200] pool2 does not need backward computation.
I0714 02:48:08.614964    11 net.cpp:200] relu2 does not need backward computation.
I0714 02:48:08.614971    11 net.cpp:200] conv2 does not need backward computation.
I0714 02:48:08.614979    11 net.cpp:200] norm1 does not need backward computation.
I0714 02:48:08.614995    11 net.cpp:200] pool1 does not need backward computation.
I0714 02:48:08.615007    11 net.cpp:200] relu1 does not need backward computation.
I0714 02:48:08.615026    11 net.cpp:200] conv1 does not need backward computation.
I0714 02:48:08.615038    11 net.cpp:200] data does not need backward computation.
I0714 02:48:08.615058    11 net.cpp:242] This network produces output loss
I0714 02:48:08.615099    11 net.cpp:255] Network initialization done.
I0714 02:48:08.615504    11 solver.cpp:172] Creating test net (#0) specified by net file: /workspace/caffe_training/caffe_models/caffe_model_2/caffenet_train_val_2.prototxt
I0714 02:48:08.615576    11 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0714 02:48:08.615897    11 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/workspace/caffe_training/input/mean.binaryproto"
  }
  data_param {
    source: "/workspace/caffe_training/input/validation_lmdb"
    batch_size: 32
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8-ones-tens"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-ones-tens"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8-ones-tens"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-ones-tens"
  bottom: "label"
  top: "loss"
}
I0714 02:48:08.616118    11 layer_factory.hpp:77] Creating layer data
I0714 02:48:08.618957    11 db_lmdb.cpp:35] Opened lmdb /workspace/caffe_training/input/validation_lmdb
I0714 02:48:08.619081    11 net.cpp:84] Creating Layer data
I0714 02:48:08.619139    11 net.cpp:380] data -> data
I0714 02:48:08.619225    11 net.cpp:380] data -> label
I0714 02:48:08.619266    11 data_transformer.cpp:25] Loading mean file from: /workspace/caffe_training/input/mean.binaryproto
I0714 02:48:08.625551    11 data_layer.cpp:45] output data size: 32,3,227,227
I0714 02:48:08.625828    11 net.cpp:122] Setting up data
I0714 02:48:08.625852    11 net.cpp:129] Top shape: 32 3 227 227 (4946784)
I0714 02:48:08.625866    11 net.cpp:129] Top shape: 32 (32)
I0714 02:48:08.625879    11 net.cpp:137] Memory required for data: 19787264
I0714 02:48:08.625893    11 layer_factory.hpp:77] Creating layer label_data_1_split
I0714 02:48:08.625923    11 net.cpp:84] Creating Layer label_data_1_split
I0714 02:48:08.625934    11 net.cpp:406] label_data_1_split <- label
I0714 02:48:08.625949    11 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0714 02:48:08.625970    11 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0714 02:48:08.626024    11 net.cpp:122] Setting up label_data_1_split
I0714 02:48:08.626057    11 net.cpp:129] Top shape: 32 (32)
I0714 02:48:08.626070    11 net.cpp:129] Top shape: 32 (32)
I0714 02:48:08.626124    11 net.cpp:137] Memory required for data: 19787520
I0714 02:48:08.626138    11 layer_factory.hpp:77] Creating layer conv1
I0714 02:48:08.626163    11 net.cpp:84] Creating Layer conv1
I0714 02:48:08.626179    11 net.cpp:406] conv1 <- data
I0714 02:48:08.626229    11 net.cpp:380] conv1 -> conv1
I0714 02:48:08.626869    11 net.cpp:122] Setting up conv1
I0714 02:48:08.626894    11 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I0714 02:48:08.626904    11 net.cpp:137] Memory required for data: 56958720
I0714 02:48:08.626919    11 layer_factory.hpp:77] Creating layer relu1
I0714 02:48:08.626947    11 net.cpp:84] Creating Layer relu1
I0714 02:48:08.626974    11 net.cpp:406] relu1 <- conv1
I0714 02:48:08.627002    11 net.cpp:367] relu1 -> conv1 (in-place)
I0714 02:48:08.627027    11 net.cpp:122] Setting up relu1
I0714 02:48:08.627035    11 net.cpp:129] Top shape: 32 96 55 55 (9292800)
I0714 02:48:08.627043    11 net.cpp:137] Memory required for data: 94129920
I0714 02:48:08.627055    11 layer_factory.hpp:77] Creating layer pool1
I0714 02:48:08.627077    11 net.cpp:84] Creating Layer pool1
I0714 02:48:08.627106    11 net.cpp:406] pool1 <- conv1
I0714 02:48:08.627122    11 net.cpp:380] pool1 -> pool1
I0714 02:48:08.627151    11 net.cpp:122] Setting up pool1
I0714 02:48:08.627163    11 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I0714 02:48:08.627176    11 net.cpp:137] Memory required for data: 103087872
I0714 02:48:08.627188    11 layer_factory.hpp:77] Creating layer norm1
I0714 02:48:08.627203    11 net.cpp:84] Creating Layer norm1
I0714 02:48:08.627213    11 net.cpp:406] norm1 <- pool1
I0714 02:48:08.627225    11 net.cpp:380] norm1 -> norm1
I0714 02:48:08.627243    11 net.cpp:122] Setting up norm1
I0714 02:48:08.627254    11 net.cpp:129] Top shape: 32 96 27 27 (2239488)
I0714 02:48:08.627269    11 net.cpp:137] Memory required for data: 112045824
I0714 02:48:08.627308    11 layer_factory.hpp:77] Creating layer conv2
I0714 02:48:08.627351    11 net.cpp:84] Creating Layer conv2
I0714 02:48:08.627364    11 net.cpp:406] conv2 <- norm1
I0714 02:48:08.627378    11 net.cpp:380] conv2 -> conv2
I0714 02:48:08.632997    11 net.cpp:122] Setting up conv2
I0714 02:48:08.633052    11 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I0714 02:48:08.633148    11 net.cpp:137] Memory required for data: 135933696
I0714 02:48:08.633185    11 layer_factory.hpp:77] Creating layer relu2
I0714 02:48:08.633334    11 net.cpp:84] Creating Layer relu2
I0714 02:48:08.633352    11 net.cpp:406] relu2 <- conv2
I0714 02:48:08.633481    11 net.cpp:367] relu2 -> conv2 (in-place)
I0714 02:48:08.633505    11 net.cpp:122] Setting up relu2
I0714 02:48:08.633519    11 net.cpp:129] Top shape: 32 256 27 27 (5971968)
I0714 02:48:08.633590    11 net.cpp:137] Memory required for data: 159821568
I0714 02:48:08.633612    11 layer_factory.hpp:77] Creating layer pool2
I0714 02:48:08.633774    11 net.cpp:84] Creating Layer pool2
I0714 02:48:08.633795    11 net.cpp:406] pool2 <- conv2
I0714 02:48:08.633808    11 net.cpp:380] pool2 -> pool2
I0714 02:48:08.633891    11 net.cpp:122] Setting up pool2
I0714 02:48:08.633929    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:08.633939    11 net.cpp:137] Memory required for data: 165359360
I0714 02:48:08.634055    11 layer_factory.hpp:77] Creating layer norm2
I0714 02:48:08.634071    11 net.cpp:84] Creating Layer norm2
I0714 02:48:08.634079    11 net.cpp:406] norm2 <- pool2
I0714 02:48:08.634093    11 net.cpp:380] norm2 -> norm2
I0714 02:48:08.634222    11 net.cpp:122] Setting up norm2
I0714 02:48:08.634366    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:08.634382    11 net.cpp:137] Memory required for data: 170897152
I0714 02:48:08.634397    11 layer_factory.hpp:77] Creating layer conv3
I0714 02:48:08.634517    11 net.cpp:84] Creating Layer conv3
I0714 02:48:08.634536    11 net.cpp:406] conv3 <- norm2
I0714 02:48:08.634814    11 net.cpp:380] conv3 -> conv3
I0714 02:48:08.657232    11 net.cpp:122] Setting up conv3
I0714 02:48:08.657299    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:08.657315    11 net.cpp:137] Memory required for data: 179203840
I0714 02:48:08.657342    11 layer_factory.hpp:77] Creating layer relu3
I0714 02:48:08.657469    11 net.cpp:84] Creating Layer relu3
I0714 02:48:08.657486    11 net.cpp:406] relu3 <- conv3
I0714 02:48:08.657505    11 net.cpp:367] relu3 -> conv3 (in-place)
I0714 02:48:08.657569    11 net.cpp:122] Setting up relu3
I0714 02:48:08.657586    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:08.657595    11 net.cpp:137] Memory required for data: 187510528
I0714 02:48:08.657603    11 layer_factory.hpp:77] Creating layer conv4
I0714 02:48:08.657636    11 net.cpp:84] Creating Layer conv4
I0714 02:48:08.657665    11 net.cpp:406] conv4 <- conv3
I0714 02:48:08.657685    11 net.cpp:380] conv4 -> conv4
I0714 02:48:08.673422    11 net.cpp:122] Setting up conv4
I0714 02:48:08.673468    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:08.673478    11 net.cpp:137] Memory required for data: 195817216
I0714 02:48:08.673492    11 layer_factory.hpp:77] Creating layer relu4
I0714 02:48:08.673508    11 net.cpp:84] Creating Layer relu4
I0714 02:48:08.673517    11 net.cpp:406] relu4 <- conv4
I0714 02:48:08.673535    11 net.cpp:367] relu4 -> conv4 (in-place)
I0714 02:48:08.673548    11 net.cpp:122] Setting up relu4
I0714 02:48:08.673611    11 net.cpp:129] Top shape: 32 384 13 13 (2076672)
I0714 02:48:08.673620    11 net.cpp:137] Memory required for data: 204123904
I0714 02:48:08.673629    11 layer_factory.hpp:77] Creating layer conv5
I0714 02:48:08.673642    11 net.cpp:84] Creating Layer conv5
I0714 02:48:08.673720    11 net.cpp:406] conv5 <- conv4
I0714 02:48:08.673735    11 net.cpp:380] conv5 -> conv5
I0714 02:48:08.685425    11 net.cpp:122] Setting up conv5
I0714 02:48:08.685482    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:08.685495    11 net.cpp:137] Memory required for data: 209661696
I0714 02:48:08.685528    11 layer_factory.hpp:77] Creating layer relu5
I0714 02:48:08.685550    11 net.cpp:84] Creating Layer relu5
I0714 02:48:08.685561    11 net.cpp:406] relu5 <- conv5
I0714 02:48:08.685628    11 net.cpp:367] relu5 -> conv5 (in-place)
I0714 02:48:08.685647    11 net.cpp:122] Setting up relu5
I0714 02:48:08.685657    11 net.cpp:129] Top shape: 32 256 13 13 (1384448)
I0714 02:48:08.685700    11 net.cpp:137] Memory required for data: 215199488
I0714 02:48:08.685711    11 layer_factory.hpp:77] Creating layer pool5
I0714 02:48:08.685727    11 net.cpp:84] Creating Layer pool5
I0714 02:48:08.685737    11 net.cpp:406] pool5 <- conv5
I0714 02:48:08.685757    11 net.cpp:380] pool5 -> pool5
I0714 02:48:08.685871    11 net.cpp:122] Setting up pool5
I0714 02:48:08.685887    11 net.cpp:129] Top shape: 32 256 6 6 (294912)
I0714 02:48:08.685900    11 net.cpp:137] Memory required for data: 216379136
I0714 02:48:08.686094    11 layer_factory.hpp:77] Creating layer fc6
I0714 02:48:08.686118    11 net.cpp:84] Creating Layer fc6
I0714 02:48:08.686242    11 net.cpp:406] fc6 <- pool5
I0714 02:48:08.686462    11 net.cpp:380] fc6 -> fc6
I0714 02:48:08.755659    15 data_layer.cpp:73] Restarting data prefetching from start.
I0714 02:48:08.814013    15 data_layer.cpp:73] Restarting data prefetching from start.
I0714 02:48:09.241827    11 net.cpp:122] Setting up fc6
I0714 02:48:09.241878    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:09.241888    11 net.cpp:137] Memory required for data: 216903424
I0714 02:48:09.241904    11 layer_factory.hpp:77] Creating layer relu6
I0714 02:48:09.241936    11 net.cpp:84] Creating Layer relu6
I0714 02:48:09.241947    11 net.cpp:406] relu6 <- fc6
I0714 02:48:09.241961    11 net.cpp:367] relu6 -> fc6 (in-place)
I0714 02:48:09.241974    11 net.cpp:122] Setting up relu6
I0714 02:48:09.241983    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:09.241991    11 net.cpp:137] Memory required for data: 217427712
I0714 02:48:09.241998    11 layer_factory.hpp:77] Creating layer drop6
I0714 02:48:09.242067    11 net.cpp:84] Creating Layer drop6
I0714 02:48:09.242074    11 net.cpp:406] drop6 <- fc6
I0714 02:48:09.242082    11 net.cpp:367] drop6 -> fc6 (in-place)
I0714 02:48:09.242094    11 net.cpp:122] Setting up drop6
I0714 02:48:09.242128    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:09.242151    11 net.cpp:137] Memory required for data: 217952000
I0714 02:48:09.242157    11 layer_factory.hpp:77] Creating layer fc7
I0714 02:48:09.242166    11 net.cpp:84] Creating Layer fc7
I0714 02:48:09.242173    11 net.cpp:406] fc7 <- fc6
I0714 02:48:09.242193    11 net.cpp:380] fc7 -> fc7
I0714 02:48:09.476433    11 net.cpp:122] Setting up fc7
I0714 02:48:09.476478    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:09.476488    11 net.cpp:137] Memory required for data: 218476288
I0714 02:48:09.476501    11 layer_factory.hpp:77] Creating layer relu7
I0714 02:48:09.476516    11 net.cpp:84] Creating Layer relu7
I0714 02:48:09.476526    11 net.cpp:406] relu7 <- fc7
I0714 02:48:09.476536    11 net.cpp:367] relu7 -> fc7 (in-place)
I0714 02:48:09.476562    11 net.cpp:122] Setting up relu7
I0714 02:48:09.476572    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:09.476579    11 net.cpp:137] Memory required for data: 219000576
I0714 02:48:09.476588    11 layer_factory.hpp:77] Creating layer drop7
I0714 02:48:09.476671    11 net.cpp:84] Creating Layer drop7
I0714 02:48:09.476680    11 net.cpp:406] drop7 <- fc7
I0714 02:48:09.476688    11 net.cpp:367] drop7 -> fc7 (in-place)
I0714 02:48:09.476694    11 net.cpp:122] Setting up drop7
I0714 02:48:09.476711    11 net.cpp:129] Top shape: 32 4096 (131072)
I0714 02:48:09.476718    11 net.cpp:137] Memory required for data: 219524864
I0714 02:48:09.476727    11 layer_factory.hpp:77] Creating layer fc8-ones-tens
I0714 02:48:09.476737    11 net.cpp:84] Creating Layer fc8-ones-tens
I0714 02:48:09.476761    11 net.cpp:406] fc8-ones-tens <- fc7
I0714 02:48:09.476773    11 net.cpp:380] fc8-ones-tens -> fc8-ones-tens
I0714 02:48:09.476953    11 net.cpp:122] Setting up fc8-ones-tens
I0714 02:48:09.476994    11 net.cpp:129] Top shape: 32 2 (64)
I0714 02:48:09.477005    11 net.cpp:137] Memory required for data: 219525120
I0714 02:48:09.477049    11 layer_factory.hpp:77] Creating layer fc8-ones-tens_fc8-ones-tens_0_split
I0714 02:48:09.477058    11 net.cpp:84] Creating Layer fc8-ones-tens_fc8-ones-tens_0_split
I0714 02:48:09.477135    11 net.cpp:406] fc8-ones-tens_fc8-ones-tens_0_split <- fc8-ones-tens
I0714 02:48:09.477187    11 net.cpp:380] fc8-ones-tens_fc8-ones-tens_0_split -> fc8-ones-tens_fc8-ones-tens_0_split_0
I0714 02:48:09.477218    11 net.cpp:380] fc8-ones-tens_fc8-ones-tens_0_split -> fc8-ones-tens_fc8-ones-tens_0_split_1
I0714 02:48:09.477253    11 net.cpp:122] Setting up fc8-ones-tens_fc8-ones-tens_0_split
I0714 02:48:09.477272    11 net.cpp:129] Top shape: 32 2 (64)
I0714 02:48:09.477280    11 net.cpp:129] Top shape: 32 2 (64)
I0714 02:48:09.477290    11 net.cpp:137] Memory required for data: 219525632
I0714 02:48:09.477306    11 layer_factory.hpp:77] Creating layer accuracy
I0714 02:48:09.477331    11 net.cpp:84] Creating Layer accuracy
I0714 02:48:09.477345    11 net.cpp:406] accuracy <- fc8-ones-tens_fc8-ones-tens_0_split_0
I0714 02:48:09.477360    11 net.cpp:406] accuracy <- label_data_1_split_0
I0714 02:48:09.477386    11 net.cpp:380] accuracy -> accuracy
I0714 02:48:09.477411    11 net.cpp:122] Setting up accuracy
I0714 02:48:09.477421    11 net.cpp:129] Top shape: (1)
I0714 02:48:09.477430    11 net.cpp:137] Memory required for data: 219525636
I0714 02:48:09.477443    11 layer_factory.hpp:77] Creating layer loss
I0714 02:48:09.477515    11 net.cpp:84] Creating Layer loss
I0714 02:48:09.477529    11 net.cpp:406] loss <- fc8-ones-tens_fc8-ones-tens_0_split_1
I0714 02:48:09.477540    11 net.cpp:406] loss <- label_data_1_split_1
I0714 02:48:09.477561    11 net.cpp:380] loss -> loss
I0714 02:48:09.477596    11 layer_factory.hpp:77] Creating layer loss
I0714 02:48:09.477634    11 net.cpp:122] Setting up loss
I0714 02:48:09.477643    11 net.cpp:129] Top shape: (1)
I0714 02:48:09.477650    11 net.cpp:132]     with loss weight 1
I0714 02:48:09.477666    11 net.cpp:137] Memory required for data: 219525640
I0714 02:48:09.477674    11 net.cpp:198] loss needs backward computation.
I0714 02:48:09.477696    11 net.cpp:200] accuracy does not need backward computation.
I0714 02:48:09.477701    11 net.cpp:198] fc8-ones-tens_fc8-ones-tens_0_split needs backward computation.
I0714 02:48:09.477710    11 net.cpp:198] fc8-ones-tens needs backward computation.
I0714 02:48:09.477718    11 net.cpp:200] drop7 does not need backward computation.
I0714 02:48:09.477726    11 net.cpp:200] relu7 does not need backward computation.
I0714 02:48:09.477735    11 net.cpp:200] fc7 does not need backward computation.
I0714 02:48:09.477751    11 net.cpp:200] drop6 does not need backward computation.
I0714 02:48:09.477764    11 net.cpp:200] relu6 does not need backward computation.
I0714 02:48:09.477777    11 net.cpp:200] fc6 does not need backward computation.
I0714 02:48:09.477790    11 net.cpp:200] pool5 does not need backward computation.
I0714 02:48:09.477802    11 net.cpp:200] relu5 does not need backward computation.
I0714 02:48:09.477808    11 net.cpp:200] conv5 does not need backward computation.
I0714 02:48:09.477815    11 net.cpp:200] relu4 does not need backward computation.
I0714 02:48:09.477826    11 net.cpp:200] conv4 does not need backward computation.
I0714 02:48:09.477838    11 net.cpp:200] relu3 does not need backward computation.
I0714 02:48:09.477847    11 net.cpp:200] conv3 does not need backward computation.
I0714 02:48:09.477867    11 net.cpp:200] norm2 does not need backward computation.
I0714 02:48:09.477882    11 net.cpp:200] pool2 does not need backward computation.
I0714 02:48:09.477895    11 net.cpp:200] relu2 does not need backward computation.
I0714 02:48:09.477952    11 net.cpp:200] conv2 does not need backward computation.
I0714 02:48:09.477967    11 net.cpp:200] norm1 does not need backward computation.
I0714 02:48:09.477980    11 net.cpp:200] pool1 does not need backward computation.
I0714 02:48:09.477993    11 net.cpp:200] relu1 does not need backward computation.
I0714 02:48:09.477998    11 net.cpp:200] conv1 does not need backward computation.
I0714 02:48:09.478006    11 net.cpp:200] label_data_1_split does not need backward computation.
I0714 02:48:09.478041    11 net.cpp:200] data does not need backward computation.
I0714 02:48:09.478096    11 net.cpp:242] This network produces output accuracy
I0714 02:48:09.478107    11 net.cpp:242] This network produces output loss
I0714 02:48:09.478152    11 net.cpp:255] Network initialization done.
I0714 02:48:09.478266    11 solver.cpp:56] Solver scaffolding done.
I0714 02:48:09.478340    11 caffe.cpp:155] Finetuning from ../bvlc_reference_caffenet.caffemodel
I0714 02:48:10.465504    11 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../bvlc_reference_caffenet.caffemodel
I0714 02:48:10.465548    11 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0714 02:48:10.465560    11 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0714 02:48:10.466152    11 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../bvlc_reference_caffenet.caffemodel
I0714 02:48:11.097406    11 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0714 02:48:11.190273    11 net.cpp:744] Ignoring source layer fc8
I0714 02:48:11.412907    11 upgrade_proto.cpp:44] Attempting to upgrade input file specified using deprecated transformation parameters: ../bvlc_reference_caffenet.caffemodel
I0714 02:48:11.412962    11 upgrade_proto.cpp:47] Successfully upgraded file specified using deprecated data transformation parameters.
W0714 02:48:11.412976    11 upgrade_proto.cpp:49] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0714 02:48:11.413012    11 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: ../bvlc_reference_caffenet.caffemodel
I0714 02:48:11.597517    11 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter
I0714 02:48:11.686884    11 net.cpp:744] Ignoring source layer fc8
I0714 02:48:11.752696    11 caffe.cpp:248] Starting Optimization
I0714 02:48:11.752761    11 solver.cpp:272] Solving CaffeNet
I0714 02:48:11.752770    11 solver.cpp:273] Learning Rate Policy: step
I0714 02:48:11.844934    11 solver.cpp:330] Iteration 0, Testing net (#0)
I0714 02:48:15.351418    15 data_layer.cpp:73] Restarting data prefetching from start.
